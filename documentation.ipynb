{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e998233-dde7-40da-ba2e-5546238304bb",
   "metadata": {},
   "source": [
    "# Identifikace Jazyka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f800eca-3747-4f59-bcda-14c8ea9272df",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49584b4f-1c98-47f3-9c05-f13b5370f1a2",
   "metadata": {},
   "source": [
    "Data jsou zpracovány z webové stránky https://www.gutenberg.org/browse/languages/nl, kde jsou vybrány data ze tří jazyků - angličtiny,holandštiny a italštiny. \n",
    "\n",
    "Data pro angličtinu byly zpracovány z webovek:\n",
    "\n",
    "https://www.gutenberg.org/cache/epub/75783/pg75783-images.html\n",
    "https://www.gutenberg.org/cache/epub/75780/pg75780-images.html\n",
    "https://www.gutenberg.org/cache/epub/8420/pg8420-images.html\n",
    "https://www.gutenberg.org/cache/epub/75781/pg75781 images.html\n",
    "https://www.gutenberg.org/cache/epub/1237/pg1237-images.html\n",
    "\n",
    "Data pro holandštinu jsou zpracovány z:\n",
    "\n",
    "https://www.gutenberg.org/cache/epub/61324/pg61324-images.html\n",
    "https://www.gutenberg.org/cache/epub/58563/pg58563-images.html\n",
    "https://www.gutenberg.org/cache/epub/48413/pg48413-images.html\n",
    "https://www.gutenberg.org/cache/epub/65697/pg65697-images.html\n",
    "https://www.gutenberg.org/cache/epub/63728/pg63728-images.html\n",
    "\n",
    "Data pro italštinu jsou zpracovány z:\n",
    "\n",
    "https://www.gutenberg.org/cache/epub/21425/pg21425-images.html\n",
    "https://www.gutenberg.org/cache/epub/38637/pg38637-images.html\n",
    "https://www.gutenberg.org/cache/epub/50674/pg50674-images.html\n",
    "https://www.gutenberg.org/cache/epub/49626/pg49626-images.html\n",
    "https://www.gutenberg.org/cache/epub/56890/pg56890-images.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b307a7ea-c0b9-474e-9f6f-0a36aa3f7523",
   "metadata": {},
   "source": [
    "## Informace o datech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb88a2cf-af02-42eb-8acc-ecb90cc50a11",
   "metadata": {},
   "source": [
    "Data jsou postupně stažená z webové stránky a čištěna od nečistých dat, kde čistě data považujeme LATIN písmena. Počet získaných tokenů na jazyk jsou rozloženy rovnoměrně a počet tokenů získané pomocí knihovny Sacremoses je po řadě jazyků 322607,322607,322620 a velikost v bytech 2860824,2582328,2875224."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa3d570-d213-47f1-829e-4c083bed39e0",
   "metadata": {},
   "source": [
    "Data byly rozděleny v poměru 80 procent,10 procent, a zbytek po řade na trénovací,heldout a testovací data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2854238-ee16-4e35-848c-45f224d9c420",
   "metadata": {},
   "source": [
    "## ngramy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bb13ce-e0ca-41d1-bfb4-ece54eb9be3c",
   "metadata": {},
   "source": [
    "Pro každý jazyk jsme vypočítali pravděpodobnost unigramů,bigramů a trigramů pomocí vzorce P = c(h,w) / c(h), kde pro unigram P = c(w) / c(všech unigramů).\n",
    "\n",
    "Pět nejčastějších trigramů angličtiny: the,and,her,hat,tha\n",
    "\n",
    "Pět nejčastějších trigramů holandštiny: een,den,aar,het,zij\n",
    "\n",
    "Pět nejčastějších trigramů italštiny: che,ell,lla,del,per"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f136decf-2ccc-40b1-83d2-4e6363c7041c",
   "metadata": {},
   "source": [
    "## Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d6c221-b83c-48db-8370-5134cca1ef48",
   "metadata": {},
   "source": [
    "Použiejem vzorec P(wIh) = ( c(w,h) + lambda) / (c(h) + lambda*V), kde lambdu zvolíme, tak aby trigramy v heldout datech měli co nejmenší cross entropii.\n",
    "\n",
    "nejlepší lambda pro angličtinu,holandštinu, je zvolena: 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e169d2a-0322-41d6-8c83-c7abe8419b19",
   "metadata": {},
   "source": [
    "## Testování"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719e3190-0c46-44ea-a41c-548045498eaa",
   "metadata": {},
   "source": [
    "Cross entropie na trénovacích datech jsou\n",
    "\n",
    "angličtina: 5.1561375174680695\n",
    "\n",
    "holandština: 4.525547780503082\n",
    "\n",
    "italština: 5.082175549172737\n",
    "\n",
    "\n",
    "A máme vytvořenou funkci Classificator.predict(languages,input_filename) na libovolně zvolený text, kde je nutné vyplnit text unknown.txt zvoleným textem. Testy na testových datech vyšli správně a jazyky byly správně přiřazeny. testový text anglický k angličtině, testový test hokandský k holandštině a testový text italský k italštině"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aa1e42-edf7-4bcd-9097-e2e9581f2482",
   "metadata": {},
   "source": [
    "## Submission File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa01a4df-e2c8-4f58-8383-6c513a0ac00c",
   "metadata": {},
   "source": [
    "your name: Jáchym Mraček\n",
    "Chosen languages: en,nl,it\n",
    "sripts: Všechny jazyky jsou v latince, kde italština a holandština mohou obshaovat přehlásky.\n",
    "data sources: Stejné jako v úvodu.\n",
    "data size in bytes: 2860824,2582328,2875224\n",
    "data size in tokens: 322607,322607,322620\n",
    "Percentage of heldout OOV tokens: 29,22,28\n",
    "Percentage of heldout OOV tokens: 29,22,28\n",
    "5 most frequent trigrams:\n",
    "\n",
    "Pět nejčastějších trigramů angličtiny: the,and,her,hat,tha\n",
    "\n",
    "Pět nejčastějších trigramů holandštiny: een,den,aar,het,zij\n",
    "\n",
    "Pět nejčastějších trigramů italštiny: che,ell,lla,del,per\n",
    "\n",
    "Best smoothing parameter values: 0.01,0.01,0.01\n",
    "\n",
    "Cross-entropy before smoothing: 3.565211720588061,3.753433935283187,3.1655068522328453\n",
    "\n",
    "Cross-entropy after smoothing: 5.1561375174680695, 4.525547780503082,5.082175549172737\n",
    "\n",
    "Random Sentence: ciao come stai, sto benissimo, come stai?\n",
    "Code output on random sentence: ('it', -52.45840302143219)\n",
    "\n",
    "OMLOUVÁM SE ://///, NĚKTERÉ INFORMACE JSEM NEUVEDL VE FORMULÁŘI SPRÁVNĚ. V KODU UVÁDÍM, ŽE JSEM NĚJAKÉ VÝPOČTY DOPLNIL PO DEADLINE, PROMIŇTE MI TO OMLOUVÁM SE. PROSÍM NEBERTE MI TOTO ŘEŠENÍ JAKO PO DEADLINE. ŘEŠENÍ PŘED DEADLINE JE: https://github.com/JachymMracek/NPFL_HW1. NAPIŠTE PROSÍM V HODNOCENÍ, JAK JSTE HODNOTILI DOPLŃUJÍCÍ INFORMACE. DĚKUJI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12630ee7-13ad-4637-ad27-eacd858f4ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from sacremoses import MosesTokenizer\n",
    "from collections import Counter\n",
    "import math\n",
    "import sys\n",
    "\n",
    "class Chars:\n",
    "    def __init__(self):\n",
    "        self.symbols = set()\n",
    "    \n",
    "    def is_symbol_in(self,char):\n",
    "        if char in self.symbols:\n",
    "            return True\n",
    "    \n",
    "    def is_correct(self,char) -> bool:\n",
    "        if char.isalpha() and unicodedata.name(char, \"\").startswith(\"LATIN\"):\n",
    "            self.symbols.add(char)\n",
    "            return True\n",
    "    \n",
    "        else:\n",
    "            return False\n",
    "\n",
    "class Formator:\n",
    "    def correct_text(filename,tag,not_print,SPACE = \" \"):\n",
    "        chars = Chars()\n",
    "\n",
    "        text = \"\"\n",
    "\n",
    "        with open(filename,\"r\" ,encoding='utf-8') as language_file:\n",
    "                for line in language_file:\n",
    "                    for char in line:\n",
    "                        \n",
    "                        if chars.is_symbol_in(char) or chars.is_correct(char):\n",
    "                            text += char\n",
    "                        \n",
    "                        elif char == SPACE:\n",
    "                            text += SPACE\n",
    "\n",
    "        tokens = Formator.__tokenize(text,tag,not_print)\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    def __tokenize(text, tag,not_print,COUNT_WORDS=322607):\n",
    "        mt = MosesTokenizer(lang=tag)\n",
    "        tokens = mt.tokenize(text)\n",
    "        \n",
    "        word_counts = Counter(tokens)\n",
    "        \n",
    "        most_common_words = word_counts.most_common()\n",
    "        \n",
    "        tokens_available = []\n",
    "        counted_words = 0\n",
    "        position = 0\n",
    "        \n",
    "        while counted_words < COUNT_WORDS:\n",
    "            if position >= len(most_common_words):\n",
    "                break\n",
    "\n",
    "            word, count = most_common_words[position]\n",
    "            \n",
    "            tokens_available.extend([word] * count)\n",
    "            counted_words += count\n",
    "            position += 1\n",
    "        \n",
    "        bytes = sys.getsizeof(tokens_available)\n",
    "\n",
    "        if not_print:\n",
    "            print(\"count bytes: \" +  str(bytes))\n",
    "            print(\"count of words: \" +  str(counted_words))\n",
    "        \n",
    "        return tokens_available\n",
    "    \n",
    "    def get_index(quotient,count_words):\n",
    "        return int((quotient*count_words) // 1)\n",
    "    \n",
    "    def split(tokens,TRAIN_QUOTIENT = 0.8,HELDOUT_QUOTIENT = 0.1):\n",
    "        count_words = len(tokens)\n",
    "\n",
    "        train_index = Formator.get_index(TRAIN_QUOTIENT,count_words)\n",
    "        heldout_index = train_index + Formator.get_index(HELDOUT_QUOTIENT,count_words)\n",
    "\n",
    "        train = tokens[:train_index + 1]\n",
    "        heldout = tokens[train_index + 1:heldout_index+1]\n",
    "        test = tokens[heldout_index+1:]\n",
    "\n",
    "        return train,heldout,test\n",
    "    \n",
    "    def get_ngrams(tokens, n):\n",
    "        ngrams = []\n",
    "\n",
    "        for token in tokens:\n",
    "            token_count = len(token)\n",
    "\n",
    "            for i in range(token_count - n + 1):\n",
    "                ngrams.append(token[i:i+n])\n",
    "                \n",
    "        return ngrams\n",
    "\n",
    "class Model:\n",
    "    N = 3\n",
    "    def __init__(self):\n",
    "        self.V = None\n",
    "        self.best_lambda = None\n",
    "        self.counter = Counter()\n",
    "    \n",
    "    def __report(self,trigrams_count,BEST=5):\n",
    "        trigrams = [(trigram, count) for trigram, count in self.counter.items() if len(trigram) == 3]\n",
    "\n",
    "        best_trigrams = sorted(trigrams, key=lambda x: x[1], reverse=True)[:BEST]\n",
    "\n",
    "        for ranking, (trigram, count) in enumerate(best_trigrams):\n",
    "            print(str(ranking + 1)  + \"th\" + \" most common trigram is \" + trigram + \n",
    "                  \" with count \" + str(count) + \" and relative frequence \" + str(round(count/trigrams_count,2)))\n",
    "    \n",
    "    def fit(self,train_tokens,tokens_heldout,tokens_test):\n",
    "        ngram_probability = {}\n",
    "        trigrams_count = None\n",
    "\n",
    "        for n in range(1,self.N+1):\n",
    "            ngrams = Formator.get_ngrams(train_tokens,n)\n",
    "            n_counter = Counter(ngrams)\n",
    "            self.counter += n_counter\n",
    "\n",
    "            self.__probability_ngram(ngrams,ngram_probability,n)\n",
    "\n",
    "            if n == self.N:\n",
    "                trigrams_count = sum(n_counter.values())\n",
    "        \n",
    "        self.__report(trigrams_count)\n",
    "\n",
    "        trigrams_heldout = Formator.get_ngrams(tokens_heldout,self.N)\n",
    "        trigrams_test = Formator.get_ngrams(tokens_test,self.N)\n",
    "\n",
    "        self.best_lambda = self.__get_best_lambda(trigrams_heldout)\n",
    "\n",
    "        self.cross_entropy_trigram_not_smoothing(ngram_probability,trigrams_test)\n",
    "\n",
    "    def cross_entropy_trigram_not_smoothing(self,ngram_probability,trigrams_test): # po deadline\n",
    "        trigrams = {trigram: count for trigram, count in ngram_probability.items() if len(trigram) == 3 and trigram in trigrams_test }\n",
    "        cross_entropy = 0\n",
    "\n",
    "        for _,p in trigrams.items():\n",
    "            cross_entropy += math.log2(p)\n",
    "\n",
    "        cross_entropy *= -1 / len(trigrams)\n",
    "\n",
    "        print(\"initial cross entropy test: \" + str(cross_entropy))\n",
    "    \n",
    "    def __probability_ngram(self,ngrams,ngram_probability,n): \n",
    "        for ngram in ngrams:\n",
    "            if n == 1:\n",
    "                history = len(self.counter)\n",
    "                self.V = history\n",
    "            else:\n",
    "                history = self.counter[ngram[:-1]]\n",
    "        \n",
    "            ngram_probability[ngram] = self.counter[ngram] / history\n",
    "\n",
    "    def __smoothing_probability(self, ngram, lambda_choosen):\n",
    "\n",
    "        if ngram not in self.counter:\n",
    "            count_ngram = 0\n",
    "        else:\n",
    "            count_ngram = self.counter[ngram]\n",
    "\n",
    "        if ngram[:-1] not in self.counter:\n",
    "            count_history = 0\n",
    "        else:\n",
    "            count_history = self.counter[ngram[:-1]]\n",
    "\n",
    "        return (count_ngram + lambda_choosen) / (count_history + lambda_choosen * self.V)\n",
    "\n",
    "    def probability_language(self, ngrams_test):\n",
    "        log_p_language = 0  # GPT Markov chain mi poddíkal na nulu.\n",
    "\n",
    "        for ngram in ngrams_test:\n",
    "            prob = self.__smoothing_probability(ngram, self.best_lambda)\n",
    "            log_p_language += math.log(prob)\n",
    "            \n",
    "        return log_p_language\n",
    "    \n",
    "    def cross_entropy(self,ngrams,lambda_choosen):\n",
    "        cross_entropy = 0\n",
    "\n",
    "        for ngram in ngrams:\n",
    "            p = self.__smoothing_probability(ngram,lambda_choosen)\n",
    "\n",
    "            if p < 0 or p > 1:\n",
    "                cross_entropy = float(\"inf\")\n",
    "            else:\n",
    "                cross_entropy += math.log2(p)\n",
    "\n",
    "        cross_entropy *= -1 / len(ngrams)\n",
    "\n",
    "        return cross_entropy\n",
    "    \n",
    "    def __get_best_lambda(self,ngrams):\n",
    "        best_lambda = None\n",
    "        best_cross_entropy = float(\"inf\")\n",
    "\n",
    "        lambdas = [x * 0.001 for x in range(1, 1000)]\n",
    "\n",
    "        for lambda_choosen in lambdas:\n",
    "            cross_entropy = self.cross_entropy(ngrams,lambda_choosen)\n",
    "        \n",
    "            if  best_cross_entropy > cross_entropy:\n",
    "                best_cross_entropy = cross_entropy\n",
    "                best_lambda = lambda_choosen\n",
    "\n",
    "        return best_lambda\n",
    "\n",
    "class Language:\n",
    "    def get_oov(self,train,trigrams): #PO deadline\n",
    "        oov = 0\n",
    "        was = 0\n",
    "        for trigram in trigrams:\n",
    "            was += 1\n",
    "            if trigram not in set(train):\n",
    "                oov += 1\n",
    "\n",
    "        print(oov / len(trigrams))\n",
    "\n",
    "    def __init__(self,tag,filename):\n",
    "        self.model = Model()\n",
    "        self.tag = tag\n",
    "\n",
    "        train_tokens,heldout_tokens,self.test_tokens = self.__read_data(tag,filename)\n",
    "\n",
    "        train_tokens_trigram = Formator.get_ngrams(train_tokens,self.model.N)\n",
    "        heldout_tokens_trigram = Formator.get_ngrams(self.test_tokens,self.model.N)\n",
    "        test_tokens_trigram = Formator.get_ngrams(self.test_tokens,self.model.N)\n",
    "\n",
    "        # self.get_oov(train_tokens_trigram,heldout_tokens_trigram) zakomentoval jsme to protože to trvá moc dlouho\n",
    "        # self.get_oov(train_tokens_trigram,test_tokens_trigram) vypočty jsou po deadline\n",
    "\n",
    "        self.model.fit(train_tokens,heldout_tokens,self.test_tokens)\n",
    "\n",
    "        print(\"Test cross entropy of language \" + tag + \" is \" + str(self.model.cross_entropy(Formator.get_ngrams(self.test_tokens,self.model.N),self.model.best_lambda)))\n",
    "    \n",
    "    def __read_data(self,tag,datafilename):\n",
    "        print(\"Language informations: \")\n",
    "        tokens = Formator.correct_text(datafilename,tag,False)\n",
    "        return Formator.split(tokens)\n",
    "\n",
    "class Classificator:\n",
    "    def predict(languages:list[Language],filename):\n",
    "        predictions = []\n",
    "\n",
    "        for language in languages:\n",
    "            tokens = Formator.correct_text(filename,language.tag,True)\n",
    "\n",
    "            ngrams = Formator.get_ngrams(tokens, Model.N)\n",
    "\n",
    "            p = language.model.probability_language(ngrams)\n",
    "\n",
    "            predictions.append((language.tag,p))\n",
    "        \n",
    "        predictions_sorted = sorted(predictions, key=lambda x: x[1], reverse=True)\n",
    "        print(predictions_sorted[0])\n",
    "\n",
    "        return predictions\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_filename = \"unknown.txt\"\n",
    "    filenames = [\"english.txt\",\"dutch.txt\",\"italian.txt\"]\n",
    "    tags = [\"en\",\"nl\",\"it\"]\n",
    "    languages = []\n",
    "\n",
    "    for i in range(3):\n",
    "        languages.append(Language(tags[i],filenames[i]))\n",
    "    \n",
    "    # for i in range(3):\n",
    "       # Classificator.predict(languages,languages[i].test_tokens)\n",
    "    \n",
    "    Classificator.predict(languages,input_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
